import torch
import torch.nn as nn
import torch.optim as optim  # Add this import
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt

# Define device for running the model (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
T = 20  # number of time steps
tau1 = 3.0  # decay constant for input filter
tau2 = 2.0  # decay constant for feedback filter
learning_rate = 0.1
num_epochs = 5
batch_size = 32
num_inputs = 784
num_hidden = 16
num_outputs = 2

# Decay filters
t = torch.arange(1, T + 1).float()
A_t = torch.exp(-t / tau1)  # exponential decay for input
B_t = -torch.exp(-t / tau2)  # negative exponential decay for feedback

# Rate encoder: generates spikes based on intensity
def rate_encode(images, T):
    rate = images.view(-1, 784).unsqueeze(-1)  # [B, 784, 1]
    rand_vals = torch.rand(rate.size(0), rate.size(1), T)
    return (rand_vals < rate).float()  # [B, 784, T]

# Load MNIST dataset (digits 0 and 1 only)
transform = transforms.ToTensor()
full_data = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
filtered_indices = [i for i, (img, label) in enumerate(full_data) if label in [0, 1]][:1000]
subset = Subset(full_data, filtered_indices)
dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0, "Embedding dim must be divisible by number of heads"
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, return_weights=False):  # <--- Make sure this method exists!
        B, N = x.size()
        Q = self.q_proj(x).view(B, self.num_heads, self.head_dim)
        K = self.k_proj(x).view(B, self.num_heads, self.head_dim)
        V = self.v_proj(x).view(B, self.num_heads, self.head_dim)

        scores = torch.einsum("bhd,bhd->bh", Q, K) / (self.head_dim ** 0.5)  # [B, H]
        attn_weights = torch.softmax(scores, dim=-1).unsqueeze(-1)  # [B, H, 1]

        attended = attn_weights * V  # [B, H, D]
        attended = attended.view(B, -1)  # Flatten: [B, embed_dim]
        out = self.out_proj(attended)

        if return_weights:
            return out, attn_weights.squeeze(-1)  # Return attention weights [B, H]
        return out


# Spiking Neural Network Model
class SNN(nn.Module):
    def __init__(self):
        super(SNN, self).__init__()
        self.attn = MultiHeadSelfAttention(embed_dim=num_inputs, num_heads=2)

        self.W1 = nn.Parameter(torch.empty(num_inputs, num_hidden))  # 784 → 16
        self.W2 = nn.Parameter(torch.empty(num_hidden, num_outputs))  # 16 → 2
        self.Wfb = nn.Parameter(torch.randn(num_outputs) * 0.01)  # 2 feedback weights
        self.bias = nn.Parameter(torch.zeros(num_outputs))  # gamma_i
        nn.init.xavier_uniform_(self.W1)
        nn.init.xavier_uniform_(self.W2)

    def forward(self, spike_train):
        B, N, T_ = spike_train.shape
        U1 = torch.zeros(B, num_hidden)
        U2 = torch.zeros(B, num_outputs, T_)  # Initialize a 3D tensor to store U2 for each time step

        A_decay = A_t.to(spike_train.device).unsqueeze(0).unsqueeze(0)  # [1,1,T]
        B_decay = B_t.to(spike_train.device).unsqueeze(0).unsqueeze(0)  # [1,1,T]

        U2_history = []

        for t in range(T):
            if t >= A_t.size(0):
                kernel_len = A_t.size(0)
                A_filtered = torch.sum(spike_train[:, :, t-kernel_len:t] * A_decay[:, :, -kernel_len:], dim=-1)
            else:
                A_filtered = torch.sum(spike_train[:, :, :t+1] * A_decay[:, :, -t-1:], dim=-1)

            attn_out = self.attn(A_filtered)  # [B, 784] → [B, 784]
            I1 = torch.matmul(attn_out, self.W1)  # [B, 16]

            U1 = I1  # no recurrent dynamics in hidden
            S1 = torch.sigmoid(U1)  # probabilistic spike from hidden layer

            I2 = torch.matmul(S1, self.W2)  # [B, 2]

            if t >= B_t.size(0):
                fb_input = torch.stack(U2_history[t-B_t.size(0):t], dim=-1)  # [B, 2, kernel]
                feedback = torch.sum(fb_input * B_decay[:, :, -B_t.size(0):], dim=-1)
            else:
                fb_input = torch.stack(U2_history, dim=-1) if U2_history else torch.zeros(B, num_outputs, 1).to(U1.device)
                feedback = torch.sum(fb_input * B_decay[:, :, -fb_input.size(-1):], dim=-1)

            U2[:, :, t] = I2 + feedback + self.bias  # Store U2 for each time step
            U2_history.append(U2[:, :, t])  # Store the membrane potentials

        return torch.sigmoid(U2)  # Apply sigmoid activation to the output membrane potentials

# --- Training Setup ---
model = SNN().to(device)  # Make sure to move the model to the device
optimizer = optim.SGD(model.parameters(), lr=learning_rate)
criterion = nn.BCELoss()

# Train the model (500 samples)
for epoch in range(num_epochs):  # Example: 10 epochs
    model.train()
    running_loss = 0.0

    for i, (inputs, targets) in enumerate(dataloader):
        optimizer.zero_grad()

        # Convert inputs to spike trains
        spike_train = rate_encode(inputs, T).to(device)
        target = torch.nn.functional.one_hot(targets, num_classes=2).float().to(device)

        # Forward pass
        outputs = model(spike_train)

        # Compute the loss
        loss = criterion(outputs[:, :, -1], target)  # Use last time step for output prediction
        loss.backward()

        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}")

# Get one batch
inputs, targets = next(iter(dataloader))
spike_train = rate_encode(inputs, T).to(device)
model.eval()

with torch.no_grad():
    # Run one forward pass manually up to attention
    B, N, T_ = spike_train.shape
    t = T_ - 1
    A_filtered = torch.sum(spike_train[:, :, :t+1] * A_t[-t-1:].to(device), dim=-1)  # shape [B, 784]
    attn_out, attn_weights = model.attn(A_filtered, return_weights=True)  # shape [B, H]

    # Plot attention weights for a single sample
    plt.figure(figsize=(6, 3))
    for h in range(attn_weights.shape[1]):
        plt.bar(h, attn_weights[0, h].item(), label=f"Head {h+1}")
    plt.title("Attention Weights (Sample 0)")
    plt.xlabel("Head")
    plt.ylabel("Weight")
    plt.legend()
    plt.tight_layout()
    plt.show()

# --- Testing Accuracy ---
model.eval()

correct = 0
total = 0

with torch.no_grad():
    for inputs, targets in dataloader:
        spike_train = rate_encode(inputs, T).to(device)
        target = targets.to(device)  # No need for one-hot encoding for comparison

        # Get the network's output
        outputs = model(spike_train)

        # Convert to predicted class (0 or 1)
        predicted = torch.argmax(outputs[:, :, -1], dim=1)  # Get the class with the highest probability

        # Compare with target class
        correct += (predicted == target).sum().item()
        total += targets.size(0)

accuracy = 100 * correct / total
print(f"Accuracy: {accuracy:.2f}%")

