import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Hyperparameters
T = 10  # time steps
TAU = 2.0 # Decay constant for input kernel A(t)
TAUm = 2.0 # Decay constant for feedback kernel B(t)
lr = 0.001 # Learning rate
epochs = 10 # Number of training epochs
batch_size = 64 #
num_inputs = 784 # MNIST 28x28 = 784 pixels
num_outputs = 2 # 2 classes: digits 0 and 1

# Decay kernels
t_vec = torch.arange(T).float()
A_t = torch.exp(-t_vec / TAU)
B_t = -torch.exp(-t_vec / TAUm)

# Data loading (only digits 0 and 1)
transform = transforms.Compose([transforms.ToTensor()])
train_set = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_set = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_mask = (train_set.targets == 0) | (train_set.targets == 1)
test_mask = (test_set.targets == 0) | (test_set.targets == 1)

train_set.targets = train_set.targets[train_mask]
train_set.data = train_set.data[train_mask]
test_set.targets = test_set.targets[test_mask]
test_set.data = test_set.data[test_mask]

train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)

# Spiking Neural Network Model
class SNN(nn.Module):
    def __init__(self):
        super(SNN, self).__init__()
        self.W_input = nn.Parameter(torch.empty(num_inputs, num_outputs))
        self.W_feedback = nn.Parameter(torch.ones(num_outputs) * 0.1)
        self.bias = nn.Parameter(torch.zeros(num_outputs))
        nn.init.xavier_uniform_(self.W_input)

    def forward(self, X, A_t, B_t):
        batch_size = X.shape[0]
        U = torch.zeros(batch_size, num_outputs, T, device=X.device)
        y_prob = torch.zeros(batch_size, num_outputs, T, device=X.device)

        for t in range(T):
            current_input = X[:, :, t] @ self.W_input + self.bias
            feedback = 0
            if t > 0:
                feedback = torch.sum(
                    y_prob[:, :, :t] * B_t[None, None, :t].flip(dims=[2]) * self.W_feedback[None, :, None],
                    dim=2
                )
            current_input += feedback

            U[:, :, t] = current_input
            y_prob[:, :, t] = torch.sigmoid(torch.clamp(current_input, -5, 5))

        return y_prob, U

# Device and model setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SNN().to(device)
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)
loss_fn = nn.BCELoss()

# Training loop
for epoch in range(epochs):
    total_loss = 0
    for images, labels in train_loader:
        images = images.view(-1, 784).to(device)
        labels = labels.to(device)

        # Rate encoding
        spikes = (torch.rand(images.shape[0], 784, T, device=device) < images.unsqueeze(-1)).float()

        # Target: spike only at middle time step
        target_spike_train = torch.zeros(images.shape[0], num_outputs, T, device=device)
        target_spike_train[torch.arange(images.shape[0]), labels, T // 2] = 1.0

        y_prob, _ = model(spikes, A_t.to(device), B_t.to(device))
        loss = loss_fn(y_prob, target_spike_train) / T

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1} | Loss: {total_loss:.4f}")

# Evaluate accuracy on test data
def evaluate(model, test_loader, A_t, B_t, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.view(-1, 784).to(device)
            labels = labels.to(device)
            spikes = (torch.rand(images.shape[0], 784, T, device=device) < images.unsqueeze(-1)).float()
            y_prob, _ = model(spikes, A_t.to(device), B_t.to(device))
            final_spike = y_prob[:, :, T // 2]  # consider middle time step
            pred = torch.argmax(final_spike, dim=1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
    print(f"Test Accuracy: {100 * correct / total:.2f}%")

evaluate(model, test_loader, A_t, B_t, device)

# Visualize prediction
import random

def show_sample_predictions(model, test_loader, A_t, B_t, device, num_samples=5):
    model.eval()
    samples_shown = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images = images.view(-1, 784).to(device)
            labels = labels.to(device)
            spikes = (torch.rand(images.shape[0], 784, T, device=device) < images.unsqueeze(-1)).float()
            y_prob, U = model(spikes, A_t.to(device), B_t.to(device))

            final_spike = y_prob[:, :, T // 2]
            pred = torch.argmax(final_spike, dim=1)

            plt.figure(figsize=(10, 3))
            plt.subplot(1, 3, 1)
            plt.imshow(images[0].view(28, 28).cpu(), cmap='gray')
            plt.title(f"True: {labels.item()} | Pred: {pred.item()}")

            plt.subplot(1, 3, 2)
            plt.plot(U[0][0].cpu(), label="Neuron 0")
            plt.plot(U[0][1].cpu(), label="Neuron 1")
            plt.title("Membrane Potential U(t)")
            plt.legend()

            plt.subplot(1, 3, 3)
            plt.plot(y_prob[0][0].cpu(), label="Neuron 0")
            plt.plot(y_prob[0][1].cpu(), label="Neuron 1")
            plt.title("Spike Probabilities")
            plt.legend()

            plt.tight_layout()
            plt.show()

            samples_shown += 1
            if samples_shown >= num_samples:
                break

show_sample_predictions(model, test_loader, A_t, B_t, device)
